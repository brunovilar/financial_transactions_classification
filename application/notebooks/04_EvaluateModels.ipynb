{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020812d5-b5a9-4f68-8948-aab89eb508bd",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "This notebook aims to evaluate the `base_model` and `modified_model` on the test set. That is the final evaluation on the unseen data, as would happen in production.\n",
    "The published versions of the models are used to assure the metrics are computed with the wrapper that would also be used in production. Thus, we avoid checking the data with an experimentation pipeline that could be different from the production code.\n",
    "\n",
    "### Tasks:\n",
    " - [ ] Load test dataset.\n",
    " - [ ] Load models:\n",
    "     - [ ] Base model.\n",
    "     - [ ] Modified model (`clothing`).\n",
    " - [ ] Evaluate models on the test set.\n",
    "     - [ ] Generate confusion matrix.\n",
    "     - [ ] Check metrics on clothing category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ab42d-6649-40e0-9005-2459a43766be",
   "metadata": {},
   "source": [
    "## Libraries and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8586e5-c7b0-4145-b093-427f6d926480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.preprocessing._label import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from application.code.core.configurations import configs\n",
    "from application.code.adapters.storage import read_dataset\n",
    "from application.code.core.model_evaluation import (compute_multiclass_classification_metrics,\n",
    "                                                    generate_feature_importance_report,\n",
    "                                                    generate_confusion_matrix,\n",
    "                                                    plot_folds_metrics)\n",
    "\n",
    "from application.code.adapters.mlflow_adapter import (get_mlflow_artifact_content,\n",
    "                                                      get_published_model,\n",
    "                                                      extract_internal_model)\n",
    "from application.code.core.feature_engineering import (format_string_columns, \n",
    "                                                       standardize_labels)\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60761921-3817-4755-b0c0-cbb0e2f06509",
   "metadata": {},
   "source": [
    "## MLflow Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab71e61-63b0-45b9-97a9-e8f9c230cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(configs.mlflow.uri)\n",
    "mlflow.set_experiment(configs.mlflow.experiment_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4160a-5e53-4b6a-b100-cde8b23d5498",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The `test` dataset is loaded to perform the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eaaec26-3dd5-47c1-82f3-1655b8ee07aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Dataset</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 1011\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Deduplicated Dataset</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 1004\n"
     ]
    }
   ],
   "source": [
    "df = read_dataset(base_path=configs.datasets.base_path, stage='raw', file_name='test')\n",
    "\n",
    "display(HTML('<h4>Dataset</h4>'))\n",
    "print(f'Records: {len(df)}')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "display(HTML('<h4>Deduplicated Dataset</h4>'))\n",
    "print(f'Records: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae287154-f98d-4581-8e8e-fda010f600bd",
   "metadata": {},
   "source": [
    "To create the model and perform experiments, only the `training` dataset will be used. The evaluation will be performed by creating some time-oriented `validation` datasets using the same methodology used to create the `test` dataset.\n",
    "\n",
    "3 sets of `training` and `validation` sets will be created, each of them representing a fold. At the end, it will be possible to have an efficacy measurement with a variance notion.\n",
    "It is important to use `validation` set avoid using the `test` several times. Ideally, it should be used only once, for the final assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14838d4-486e-4175-894b-7dd70f6b5f7d",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "The models are retrieved from MLflow server to be used as it would in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6af819-5041-4f91-940d-41398be22554",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = get_published_model(model_name=configs.mlflow.base_model_name,\n",
    "                                 stage=\"Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7391b81-f0a7-41f2-bebb-dd7948c5bc46",
   "metadata": {},
   "source": [
    "As MLFlow only exposes the `predict` function, it is necessary to extract the internal model to have access to all the developed functions. These functions are necessary to perform some low level operations to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c70e5e-4791-4bd6-9b49-4301eed9fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_model = extract_internal_model(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9b227-69e3-42bb-b5e5-98ee91cbfa9d",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Compute predictions and encode labels to be able to compare predictions with ground truth labels and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d069892-e64d-466f-ba4d-862d41b823b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 1004\n",
      "Sample: compra online, serviço, artigos eletro, artigos eletro, serviço\n",
      "\n",
      "Encoded Predictions: 1004\n",
      "Sample: 5, 16, 2, 2, 16\n"
     ]
    }
   ],
   "source": [
    "predictions = base_model.predict(df)\n",
    "\n",
    "print(f'Predictions: {len(predictions)}')\n",
    "print(f'Sample: {\", \".join(predictions[:5])}')\n",
    "\n",
    "encoded_predictions = internal_model.encode_labels(predictions)\n",
    "\n",
    "print(f'\\nEncoded Predictions: {len(encoded_predictions)}')\n",
    "print(f'Sample: {\", \".join(map(str, encoded_predictions[:5]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e1bab-198a-48fa-83f4-292812dfbff7",
   "metadata": {},
   "source": [
    "Preprocess and encode raw labels to be able to compare with the model generated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cb342e-7456-4083-88d0-64050015cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 1004\n",
      "Sample: ARTIGOS ELETRO, M.O.T.O., M.O.T.O., ARTIGOS ELETRO, SERVIO\n",
      "\n",
      "Encoded Labels: 1004\n",
      "Sample: 2, 5, 5, 2, 16\n"
     ]
    }
   ],
   "source": [
    "labels = df['grupo_estabelecimento'].to_list()\n",
    "\n",
    "print(f'Labels: {len(labels)}')\n",
    "print(f'Sample: {\", \".join(map(str, labels[:5]))}')\n",
    "\n",
    "encoded_labels = internal_model.encode_labels(labels)\n",
    "\n",
    "print(f'\\nEncoded Labels: {len(encoded_labels)}')\n",
    "print(f'Sample: {\", \".join(map(str, encoded_labels[:5]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c731b-6631-49a9-a7d0-09acec751887",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55401059-8b9a-4b4e-8686-3806054b17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_multiclass_classification_metrics(encoded_labels, encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b749ccfc-9107-4025-be8b-61d9d09569d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macro_precision</td>\n",
       "      <td>0.204147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macro_recall</td>\n",
       "      <td>0.173658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>macro_f1</td>\n",
       "      <td>0.171323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>micro_precision</td>\n",
       "      <td>0.397410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>micro_recall</td>\n",
       "      <td>0.397410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro_f1</td>\n",
       "      <td>0.397410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted_precision</td>\n",
       "      <td>0.379709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_recall</td>\n",
       "      <td>0.397410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighted_f1</td>\n",
       "      <td>0.381886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metric     value\n",
       "0     macro_precision  0.204147\n",
       "1        macro_recall  0.173658\n",
       "2            macro_f1  0.171323\n",
       "3     micro_precision  0.397410\n",
       "4        micro_recall  0.397410\n",
       "5            micro_f1  0.397410\n",
       "6  weighted_precision  0.379709\n",
       "7     weighted_recall  0.397410\n",
       "8         weighted_f1  0.381886"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = (\n",
    "    pd\n",
    "    .DataFrame([metrics])\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .set_axis(['metric', 'value'], axis=1)\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207f763-e8dd-42fe-9da7-48f9c0977c2b",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d446d2-40ed-4a44-9c06-e5caa092a910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
